# -*- coding: utf-8 -*-
"""Finetuning_SFT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MEt-V-pU06LpUDbCh5nFaVV31u3UoH04
"""

import torch

torch.cuda.is_available()

##Implementing supervised fine tuning using ultra chat
!pip install datasets
!pip install transformers
##trl is transformers reiforcement learning which helps with the implmentation of reinforcement learning on the transformer arch
!pip install -q bitsandbytes trl peft

!pip install flash-attn --no-build-isolation
##we will be using the T4 flash-attn 1 for now

from datasets import load_dataset
raw_datasets = load_dataset("HuggingFaceH4/ultrachat_200k")

from datasets import DatasetDict
indices = range(0,100)
dataset_dict = {"train": raw_datasets["train_sft"].select(indices), "test": raw_datasets["test_sft"].select(indices)}
dataset_dict
raw_datasets = DatasetDict(dataset_dict)
raw_datasets

example = raw_datasets["train"][0]
example

messages = example["messages"]
for message in messages:
  role = message["role"]
  content = message["content"]
  print(f"{role}: {content}\n")

from transformers import AutoTokenizer

model_id = "mistralai/Mistral-7B-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_id)

text= "Ola, how is your day going beautiful?"
tokens = tokenizer.tokenize(text)
tokens
input_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids
print(tokenizer.decode(input_ids))

# set pad_token_id equal to the eos_token_id if not set
if tokenizer.pad_token_id is None:
  tokenizer.pad_token_id = tokenizer.eos_token_id

# Set reasonable default for models without max length
if tokenizer.model_max_length > 100_000:
  tokenizer.model_max_length = 2048

# Set chat template
DEFAULT_CHAT_TEMPLATE = "{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'system' %}\n{{ '<|system|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}"
tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE

##applying the default chat template
import re
import random
from multiprocessing import cpu_count

def apply_chat_template(example, tokenizer):
  messages = example["messages"]
  ##adding a system message if there are no messages rn
  if messages[0]["role"] != "system":
    messages.insert(0, {"role": "system", "content": ""})
  example["text"] = tokenizer.apply_chat_template(messages, tokenize=False)
  return example

column_names = list(raw_datasets["train"].features)
raw_datasets = raw_datasets.map(apply_chat_template,
                                num_proc=cpu_count(),
                                fn_kwargs={"tokenizer": tokenizer},
                                remove_columns=column_names,
                                desc="Applying chat template",)

# create the splits
train_dataset = raw_datasets["train"]
eval_dataset = raw_datasets["test"]

for index in random.sample(range(len(raw_datasets["train"])), 3):
  print(f"Sample {index} of the processed training set:\n\n{raw_datasets['train'][index]['text']}")

##Now we will actually look into the aspects of fine tuning
## to begin with their is full finetuning(fine tune every single weight in a full precision way float32), LoRA(fine tune only some weights known as adapter effecient method (PEFT) where we take the low rank of the matrices which use less space to fine tune ) and QLoRA.
##QLoRA - Combines Quantization and Low Rank Adaptation technique in order to decrease the size and keep the performance the same

from transformers import BitsAndBytesConfig
import torch

##quantizing the model
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

device_map = {"": torch.cuda.current_device() if torch.cuda.is_available() else "cpu"}

model_kwargs = dict(
    attn_implementation="flash_attention_2",
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    quantization_config=quantization_config,
    device_map=device_map,
)

model_kwargs

from trl import SFTTrainer
from peft import LoraConfig
from transformers import TrainingArguments



from trl import SFTTrainer
from peft import LoraConfig
from transformers import TrainingArguments

# path where the Trainer will save its checkpoints and logs
output_dir = 'data/zephyr-7b-sft-lora'

# based on config
training_args = TrainingArguments(
    fp16=True, # specify bf16=True instead when training on GPUs that support bf16
    do_eval=True,
    evaluation_strategy="epoch",
    gradient_accumulation_steps=128,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},
    learning_rate=2.0e-05,
    log_level="info",
    logging_steps=5,
    logging_strategy="steps",
    lr_scheduler_type="cosine",
    max_steps=-1,
    num_train_epochs=1,
    output_dir=output_dir,
    overwrite_output_dir=True,
    per_device_eval_batch_size=1, # originally set to 8
    per_device_train_batch_size=1, # originally set to 8
    # push_to_hub=True,
    # hub_model_id="zephyr-7b-sft-lora",
    # hub_strategy="every_save",
    # report_to="tensorboard",
    save_strategy="no",
    save_total_limit=None,
    seed=42,
)

# based on config
peft_config = LoraConfig(
        r=64,
        lora_alpha=16,
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
)

trainer = SFTTrainer(
        model=model_id,
        model_init_kwargs=model_kwargs,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        dataset_text_field="text",
        tokenizer=tokenizer,
        packing=True,
        peft_config=peft_config,
        max_seq_length=tokenizer.model_max_length,
    )

train_result = trainer.train()

trainer.save_model()